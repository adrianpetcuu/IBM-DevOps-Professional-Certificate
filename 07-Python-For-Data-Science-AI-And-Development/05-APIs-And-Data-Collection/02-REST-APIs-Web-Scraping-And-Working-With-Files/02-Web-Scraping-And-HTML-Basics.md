# ğŸ•¸ï¸ Web Scraping & HTML Basics (Ã®n Python)

Acest ghid practic te Ã®nvaÈ›Äƒ **bazele HTML** È™i **cum sÄƒ faci web scraping** Ã®n Python Ã®n mod **etic, robust È™i scalabil** â€” folosind `requests`, `BeautifulSoup` È™i, cÃ¢nd e nevoie, `Selenium`.

---

## ğŸ¯ Obiective
DupÄƒ parcurgere vei putea:
- ÃnÈ›elege structura unui document **HTML** (tag-uri, atribute, DOM).
- Extrage date cu **CSS Selectors** È™i **XPath**.
- Trimite cereri HTTP cu **`requests`** È™i gestionezi **headere, timeouts, retries, sesiuni**.
- Parsezi pagini cu **BeautifulSoup** sau **lxml**.
- Navighezi **pagination**, descarci imagini/fiÈ™iere È™i salvezi datele Ã®n **CSV/JSON**.
- CunoÈ™ti regulile de **eticÄƒ & legalitate** (robots.txt, TOS, rate limiting).

---

## ğŸ§± 1) HTML â€” noÈ›iuni de bazÄƒ

Un document HTML este format din **tag-uri** (ex: `<div>`, `<a>`, `<p>`) care pot avea **atribute** (ex: `class`, `id`, `href`).

```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Exemplu</title>
  </head>
  <body>
    <h1 class="title">Titlul paginii</h1>
    <a href="/produs/123" data-price="49.90">Vezi produs</a>
    <ul id="items">
      <li class="item">Element 1</li>
      <li class="item">Element 2</li>
    </ul>
  </body>
</html>
```

**DOM** (Document Object Model) = arborele elementelor HTML.  
Extragerea datelor se face selectÃ¢nd elemente din DOM dupÄƒ **tag**, **clasÄƒ**, **id** etc.

---

## ğŸ¯ 2) Selectori â€” CSS & XPath

### CSS Selectors (uÈ™or de folosit cu BeautifulSoup)
- `h1` â€“ toate h1-urile
- `.item` â€“ toate elementele cu clasa `item`
- `#items` â€“ elementul cu id `items`
- `ul#items > li.item` â€“ li cu clasa `item` doar copii direcÈ›i ai lui `ul#items`
- `a[href^="/produs/"]` â€“ link-uri a cÄƒror `href` Ã®ncepe cu `/produs/`

### XPath (puternic cu lxml/Selenium)
- `//h1` â€“ toate h1-urile
- `//*[@id="items"]/li[@class="item"]` â€“ li cu clasa `item` sub id `items`
- `//a[starts-with(@href, '/produs/')]` â€“ link-uri cu href ce Ã®ncepe cu `/produs/`
- `string(//h1)` â€“ textul din primul h1

---

## ğŸŒ 3) HTTP Requests Ã®n scraping

### De ce sÄƒ foloseÈ™ti `requests` cu grijÄƒ
- SeteazÄƒ **`timeout`** pentru a nu bloca programul.
- FoloseÈ™te **`Session()`** pentru conexiuni reutilizabile (mai rapid).
- AdaugÄƒ **User-Agent** pentru a mima un browser real.
- RespectÄƒ **rate limiting** (sleep Ã®ntre cereri).

```python
import time
import requests

BASE = "https://exemplu.tld"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/127.0 Safari/537.36"
}

with requests.Session() as s:
    s.headers.update(headers)
    for page in range(1, 4):
        url = f"{BASE}/catalog?page={page}"
        resp = s.get(url, timeout=15)
        resp.raise_for_status()
        print(f"[{resp.status_code}] {url}")
        time.sleep(1.0)  # respectare rate limiting
```

---

## ğŸœ 4) Parsare HTML cu BeautifulSoup

```python
from bs4 import BeautifulSoup

html = """
<ul id="items">
  <li class="item">Element 1</li>
  <li class="item">Element 2</li>
</ul>
"""

soup = BeautifulSoup(html, "html.parser")

# prin tag
print([li.text for li in soup.find_all("li")])

# dupÄƒ clasÄƒ
print([li.text for li in soup.select("li.item")])

# dupÄƒ id + copil
ul = soup.select_one("ul#items")
print([li.get_text(strip=True) for li in ul.select("> li.item")])
```

### Extrage atribute
```python
html = '<a href="/produs/123" data-price="49.90">Vezi produs</a>'
soup = BeautifulSoup(html, "html.parser")
a = soup.select_one("a")
print(a["href"], a.get("data-price"))
```

---

## âš¡ 5) Parsare rapidÄƒ cu lxml (XPath)

```python
from lxml import html as lhtml

doc = lhtml.fromstring("""
<ul id="items">
  <li class="item">Element 1</li>
  <li class="item">Element 2</li>
</ul>
""")

items = doc.xpath('//*[@id="items"]/li[@class="item"]/text()')
print(items)  # ['Element 1', 'Element 2']
```

---

## ğŸ” 6) Pagination + extragere listÄƒ detalii

Exemplu generic pentru listÄƒ â†’ intrÄƒ pe fiecare paginÄƒ de detaliu:

```python
import time
import csv
import requests
from bs4 import BeautifulSoup

BASE = "https://exemplu.tld"
OUT = "produse.csv"
headers = {"User-Agent": "Mozilla/5.0 ..."}

def parse_list(html):
    soup = BeautifulSoup(html, "html.parser")
    # Link-uri cÄƒtre produsele individuale
    links = [a["href"] for a in soup.select("a.product-card[href]")]
    # NormalizeazÄƒ link-urile relative
    return [l if l.startswith("http") else BASE + l for l in links]

def parse_detail(html):
    soup = BeautifulSoup(html, "html.parser")
    title = soup.select_one("h1.product-title").get_text(strip=True)
    price = soup.select_one(".price .amount").get_text(strip=True)
    sku = soup.select_one("[data-sku]").get("data-sku")
    return {"title": title, "price": price, "sku": sku}

with requests.Session() as s, open(OUT, "w", newline="", encoding="utf-8") as f:
    s.headers.update(headers)
    writer = csv.DictWriter(f, fieldnames=["title", "price", "sku"])
    writer.writeheader()

    for page in range(1, 4):
        list_url = f"{BASE}/catalog?page={page}"
        r = s.get(list_url, timeout=15); r.raise_for_status()
        for url in parse_list(r.text):
            rd = s.get(url, timeout=15); rd.raise_for_status()
            data = parse_detail(rd.text)
            writer.writerow(data)
            time.sleep(0.8)
        time.sleep(1.0)

print(f"Scris Ã®n {OUT}")
```

---

## ğŸ§² 7) DescÄƒrcat imagini / fiÈ™iere

```python
import os, pathlib, requests

IMG_URL = "https://exemplu.tld/img/produs123.jpg"
IMG_DIR = pathlib.Path("imagini"); IMG_DIR.mkdir(exist_ok=True)

with requests.get(IMG_URL, stream=True, timeout=20) as r:
    r.raise_for_status()
    dest = IMG_DIR / os.path.basename(IMG_URL)
    with open(dest, "wb") as f:
        for chunk in r.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
print("Salvat:", dest)
```

---

## ğŸ§ª 8) CÃ¢nd ai nevoie de Selenium (È™i alternative)

FoloseÈ™te **Selenium** dacÄƒ:
- Site-ul **Ã®ncarcÄƒ conÈ›inutul dinamic** (JS) È™i nu existÄƒ endpoint JSON uÈ™or de chemat.
- Trebuie sÄƒ **simulezi interacÈ›iuni** (click, scroll, login 2FA).

Exemplu minim cu Selenium (Chrome headless):

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

opts = Options()
opts.add_argument("--headless=new")
driver = webdriver.Chrome(options=opts)

try:
    driver.get("https://exemplu.tld")
    h1 = driver.find_element(By.CSS_SELECTOR, "h1.title").text
    print(h1)
finally:
    driver.quit()
```

**Alternative mai rapide** (cÃ¢nd se poate):  
- CautÄƒ **endpoint-uri XHR/JSON** Ã®n DevTools â†’ apeleazÄƒ-le direct cu `requests`.  
- FoloseÈ™te **Playwright** (mai rapid/stabil decÃ¢t Selenium Ã®n unele scenarii).

---

## ğŸ§± 9) RobustezÄƒ: retries, timeouts, proxies, anti-bot

### Retries + backoff
```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

session = requests.Session()
retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])
session.mount("https://", HTTPAdapter(max_retries=retries))
session.headers["User-Agent"] = "Mozilla/5.0 ..."
resp = session.get("https://exemplu.tld", timeout=15)
```

### Proxies & User-Agent rotation
- FoloseÈ™te **liste de user-agents** È™i **proxy rotativ** pentru protecÈ›ie anti-bot.
- Nu abuza â€” vezi secÈ›iunea de eticÄƒ.

---

## ğŸ’¾ 10) Salvare date (CSV, JSON, Parquet)

```python
import csv, json

rows = [{"title": "Produs A", "price": "49.90"}, {"title": "Produs B", "price": "79.00"}]

# CSV
with open("out.csv", "w", newline="", encoding="utf-8") as f:
    w = csv.DictWriter(f, fieldnames=rows[0].keys())
    w.writeheader(); w.writerows(rows)

# JSON
with open("out.json", "w", encoding="utf-8") as f:
    json.dump(rows, f, ensure_ascii=False, indent=2)
```

Pentru volume mari â†’ **Parquet** (pandas + pyarrow).

---

## ğŸ“ 11) EticÄƒ, Legal È™i Bune Practici

- **RespectÄƒ `robots.txt`** (ex: `https://site.tld/robots.txt`) È™i **Termenii & CondiÈ›iile** site-ului.
- **Nu supraÃ®ncarcÄƒ** serverele. FoloseÈ™te `time.sleep()`, `rate limiting`, `If-Modified-Since` / `ETag`.
- **IdentificÄƒ-te** politicos prin User-Agent (ex.: include e-mailul tÄƒu, dacÄƒ e potrivit).
- **Nu colecta date personale** fÄƒrÄƒ bazÄƒ legalÄƒ (GDPR/consimÈ›ÄƒmÃ¢nt).
- **Cache local** pentru a evita cereri repetate.
- DacÄƒ existÄƒ **API oficial**, **foloseÈ™te-l** Ã®n locul scraping-ului.

---

## ğŸ§° 12) Debugging & DevTools (Chrome/Firefox)
- InspecteazÄƒ elementele (Right click â†’ Inspect).
- Tab-ul **Network** â†’ vezi cereri XHR, endpoints JSON, headere, cookies.
- **Copy selector** / **Copy XPath** pe elementul dorit.
- ÃnÈ›elege **lazy-loading** (scroll pentru Ã®ncÄƒrcare).

---

## âœ… Checklist de proiect
- [ ] Verificat `robots.txt` + TOS
- [ ] Definite È›intele (ce cÃ¢mpuri extrag?)
- [ ] Stabilit selecÈ›ii CSS/XPath rezistente
- [ ] AdÄƒugat `Session()`, `timeout`, `retries`, `backoff`
- [ ] Rate limiting + random delay
- [ ] Salvat Ã®n format potrivit (CSV/JSON/DB)
- [ ] Loguri & erori tratate
- [ ] Testat pe eÈ™antion, apoi rulat Ã®n batch

---

## ğŸ“š Resurse utile
- **BeautifulSoup**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **lxml**: https://lxml.de/
- **Requests**: https://requests.readthedocs.io/
- **Selenium**: https://www.selenium.dev/documentation/
- **Playwright**: https://playwright.dev/python/
- **XPath Cheatsheet**: https://devhints.io/xpath

---

> **Concluzie:** Ãncepe cu `requests` + `BeautifulSoup` pentru site-uri statice, treci la **XPath/lxml** pentru vitezÄƒ È™i la **Selenium/Playwright** doar cÃ¢nd conÈ›inutul e Ã®ncÄƒrcat dinamic. RespectÄƒ regulile de bunÄƒ conduitÄƒ È™i construieÈ™te parseri rezistenÈ›i la schimbÄƒri.
